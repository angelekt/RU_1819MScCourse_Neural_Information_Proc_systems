{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlqVBEfLBOF_"
   },
   "source": [
    "**SOW-MKI49: Neural Information Processing Systems**  \n",
    "*Weeks 2 and 3: Assignment (200 points + 20 bonus points + 1 bonus point for each bug you find and another bonus point if you debug it and before you ask, no, typos unfortunately are not considered bugs - first come, first served)*  \n",
    "Author: Umut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT8BaKk5CpeB"
   },
   "outputs": [],
   "source": [
    "# Group number: 14\n",
    "# Veerle Schepers s1023102\n",
    "# Angeliki-Ilektra Karaiskou, s1029746\n",
    "# Lei Xiaoxuan, s1025681"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00iIAIv37Del"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "data_directory = 'C:\\\\Users\\\\Αngeliki-Ilektra\\\\NeuralInformationProcessingSystems\\\\1stAs\\\\Data_directory'\n",
    "                    # Make a directory to store the data and enter it here.\n",
    "                    # We will be using a smaller dataset (LFW) than the one used in the paper (CelebA) for computational resource considerations.\n",
    "                    # Download it from http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz.\n",
    "device = -1\n",
    "epochs = 4\n",
    "lambda_ = {'feature': 1., 'pixel': 1., 'total_variation': 1e-5}\n",
    "model_directory = 'C:\\\\Users\\\\Αngeliki-Ilektra\\\\NeuralInformationProcessingSystems\\\\1stAs\\\\modelDirectory'\n",
    "# Make a directory to store the models and enter it here. Move Vgg4Layers.npz to the model directory.\n",
    "outsize = (96, 96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86zZctPu1K2M"
   },
   "source": [
    "**Packages (10 points)**  \n",
    "In this cell, you will import the required packages.  \n",
    "*Tasks*   \n",
    "- (1) It is always good practice to first think about the big picture and not rush into writing code before clearly knowing everything that you will have to do so as to avoid future complications. Therefore, your first task is to study the skeleton code and come up with a plan of how to proceed. (**0 points**)\n",
    "- (2) However, I agree that doing so is arguably the most boring part of coding, and you rather skip it. To help you to resist the temptation of skipping going through the skeleton code, I have removed the import statements. Your second task is to Identify the required packages and import them. Note that if you are using Python 2.7, you should import print from the future. (**10 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RBiJw5pV030o"
   },
   "outputs": [],
   "source": [
    "# (2) start\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer.backends import cuda\n",
    "from chainer import Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "from resizeimage import resizeimage\n",
    "\n",
    "# (2) end\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "from chainer.optimizers import Adam\n",
    "from chainer.backends import cuda\n",
    "import glob\n",
    "from glob import glob\n",
    "from chainer.serializers import load_npz, save_npz\n",
    "from pprint import pprint\n",
    "from chainer.dataset import concat_examples\n",
    "from matplotlib import pyplot as plt\n",
    "from chainer.dataset import DatasetMixin\n",
    "\n",
    "#skiimage package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "\n",
    "#psnr\n",
    "from skimage.measure import compare_psnr as psnr\n",
    "#ssim\n",
    "from skimage.measure import compare_ssim as ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOhjvsOx9lJY"
   },
   "source": [
    "**Preprocessing functions (10 points + 5 bonus points)** (taken from https://github.com/mbeyeler/opencv-python-blueprints)  \n",
    "In the following cell, you will implement some of the preprocessing functions. The rest of the preprocessing steps have already been applied to the data.  \n",
    "*Tasks*\n",
    "- (1) Implement the resizing operation. That is, you should extract the data, resize each portrait to 96 pixels x 96 pixels and save them to the data directory as JPG. (**10 points **)\n",
    "- (2) The pencil sketch class implements the sketch effect in a simpler way than the one mentioned in the lecture. Explain how/why the used operations (blur and divide) convert portraits to sketches, and how it differs from that which was mentioned in the lecture? (**5 bonus points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iY4lbpLK9kp4"
   },
   "outputs": [],
   "source": [
    "# (1) start resize image\n",
    "# create a new folder to store the resized images\n",
    "dirName = './Data_directory/Resize'\n",
    "\n",
    "\n",
    "# If the folder already exist (from a previous running), delete the folder and it's content\n",
    "if os.path.exists(dirName):\n",
    "    shutil.rmtree(dirName)\n",
    "else:    ## Show an error ##\n",
    "    print('Error')\n",
    "os.makedirs(dirName)   \n",
    "\n",
    "\n",
    "# list all the names of the folder in lfw_deepfunneled\n",
    "data_f=os.listdir(data_directory + '/lfw_deepfunneled')\n",
    "#print data_f\n",
    "\n",
    "# create path \n",
    "path = os.path.join(data_directory, 'lfw_deepfunneled')\n",
    "#print path\n",
    "\n",
    "# for every indivdual map\n",
    "for img_f in data_f:\n",
    "    # create path to, and a list of all the folders in lfw_deepfunneled\n",
    "    path2 = os.path.join(path, img_f);\n",
    "    data_f2 = os.listdir(path2)\n",
    "    #print data_f2\n",
    "    \n",
    "    # for every individual image\n",
    "    for img_f2 in data_f2:\n",
    "        # create path to, and a list of all the images in the personal map\n",
    "        path3 = os.path.join(path,img_f, img_f2)\n",
    "        Parh3 = path3.replace('\\\\', '/')\n",
    "        # print path3\n",
    "        \n",
    "        r_img = Image.open(path3)\n",
    "        #r_img.show()\n",
    "        r_img = r_img.resize((96, 96), Image.ANTIALIAS)\n",
    "        r_img.save(data_directory+ '/resize_'+ img_f2);\n",
    "# (1) end'''\n",
    "\n",
    "class PencilSketch:\n",
    "    \"\"\"Pencil sketch effect\n",
    "        A class that applies a pencil sketch effect to an image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimension):\n",
    "        \"\"\"Initialize parameters\n",
    "            :param (width, height): Image size.\n",
    "        \"\"\"\n",
    "        self.width,self.height=dimension\n",
    "        \n",
    "\n",
    "\n",
    "    def render(self, img_rgb):\n",
    "        \"\"\"Applies pencil sketch effect to an RGB image\n",
    "            :param img_rgb: RGB image to be processed\n",
    "            :returns: Processed RGB image\n",
    "        \"\"\"\n",
    "        img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "        img_blur = cv2.GaussianBlur(img_gray, (21, 21), 0, 0)\n",
    "        img_blend = cv2.divide(img_gray, img_blur, scale=256)\n",
    "\n",
    "        # return cv2.cvtColor(img_blend, cv2.COLOR_GRAY2RGB)\n",
    "        return img_blend\n",
    "\n",
    "def pencil_sketch(img_rgb):\n",
    "    pencilSketch = PencilSketch((img_rgb.shape[1], img_rgb.shape[0]))\n",
    "\n",
    "    return pencilSketch.render(img_rgb)\n",
    "\n",
    "# (2) The operation first converst the colored image to a grayscale image to work on one layer\n",
    "# In the next step, this gray image is blurred. This creates a smooth image with a low amount of noise\n",
    "# Dividing the grayscale image by the blurred images rusults in an image in which only the parts\n",
    "# where the original image differs much from the blurred images, which is a the edges, will\n",
    "# be present. This gives a line sketch result.\n",
    "# In the lecture a method in which 3 different skatches, namely a linesketch, grayscale sketch,\n",
    "# and color sketch are created. \n",
    "# For the method mentioned in the paper is based on Gaussian kernal but for pencilSketch from cv2 \n",
    "# is based on bilateral filtering\n",
    "\n",
    "def preprocess(img):\n",
    "    if img.mode == 'L':\n",
    "        return np.rollaxis(np.asarray(img, 'float32')[..., None], 2)\n",
    "    else:\n",
    "        return np.rollaxis(np.asarray(img, 'float32')[..., ::-1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovQMUuo_7D2k"
   },
   "source": [
    "**Data class**  \n",
    "The following cell defines the data class. It is used to manage the data (loading, etc.). *You do not have to make any changes to the code.*  \n",
    "*Task*\n",
    "- (1) Study the code and refer to the chainer docuimentation if anything is unclear. You will be expected to write similar code by yourself in the coming practicals. (**0 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OF39paH6wff"
   },
   "outputs": [],
   "source": [
    "class Dataset(DatasetMixin):\n",
    "    def __init__(self, data_files):\n",
    "        self.data_files = data_files\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.data_files)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        t = np.asarray(Image.open(self.data_files[i]).convert('RGB').resize((96, 96), Image.LANCZOS), 'f').transpose(2, 0, 1)\n",
    "\n",
    "        x = pencil_sketch(np.asarray((Image.open(self.data_files[i]).convert('RGB').resize((96, 96), Image.LANCZOS)), 'f'))[None]\n",
    "\n",
    "        return t, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUjEFdDD6xBq"
   },
   "source": [
    "**Model classes (45 points)**  \n",
    "In the following cellyou will implement the model classes.\n",
    "*Tasks*   \n",
    "- (1) Implement the layers of the model by filling in the missing code. (**20 points**)\n",
    "- (2) Reimplement the model as a ChainList instead of a Chain. (**5 points**)\n",
    "- (3) Implement the forward pass of the residual block by filling in the missing code. (**20 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nafY2Wgx6QLt"
   },
   "outputs": [],
   "source": [
    "class Model(Chain):\n",
    "    def __init__(self, in_channels, outsize):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            convolution2D_0=L.Convolution2D(1,32,9,pad = 4, nobias = True)\n",
    "            batchNormalization_0 = L.BatchNormalization(32)\n",
    "            convolution2D_1 = L.Convolution2D(32,64,3, stride=2, pad=1,nobias=True)\n",
    "            batchNormalization_1 = L.BatchNormalization(64)\n",
    "            convolution2D_2 = L.Convolution(64,128,3,stride=2,pad=1,nobias=True)\n",
    "            batchNormalization_2 = L.BatchNormalization(128)\n",
    "            residualBlock_3 = ResidualBlock(3,128,128,1)\n",
    "            residualBlock_4 = ResidualBlock(3,128,128,1)\n",
    "            residualBlock_5 = ResidualBlock(3,128,128,1)\n",
    "            residualBlock_6 = ResidualBlock(3,128,128,1)\n",
    "            residualBlock_7 = ResidualBlock(3,128,128,1)\n",
    "            deconvolution2D_9 = L.Deconvolution2D(128, 64, 3, 2, 1, True, outsize)\n",
    "            batchNormalization_9 = L.BatchNormalization(64)\n",
    "            deconvolution2D_10 = L.Deconvolution2D(64, 32, 3, 2, 1, True, outsize)\n",
    "            batchNormalization_10 = L.BatchNormalization(32)\n",
    "            convolution2D_11 = L.Convolution2D(32, 3, 9, pad = 4, nobias = True)\n",
    "            batchNormalization_11 = L.BatchNormalization(3)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.outsize = outsize\n",
    "\n",
    "    def __call__(self, x, finetune = False):\n",
    "        h = self.convolution2D_0(x)\n",
    "        h = self.batchNormalization_0(h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_1(h)\n",
    "        h = self.batchNormalization_1(h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_2(h)\n",
    "        h = self.batchNormalization_2(h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.residualBlock_3(h, finetune)\n",
    "        h = self.residualBlock_4(h, finetune)\n",
    "        h = self.residualBlock_5(h, finetune)\n",
    "        h = self.residualBlock_6(h, finetune)\n",
    "        h = self.residualBlock_7(h, finetune)\n",
    "        h = self.deconvolution2D_8(h)\n",
    "        h = self.batchNormalization_8(h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.deconvolution2D_9(h)\n",
    "        h = self.batchNormalization_9(h, finetune)\n",
    "        h = self.deconvolution2D_10(h)\n",
    "        h = self.batchNormalization_10(h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_11(h)\n",
    "        h = self.batchNormalization_11(h, finetune)\n",
    "        y = 127.5 * F.tanh(h) + 127.5\n",
    "\n",
    "        return y\n",
    "\n",
    "class Model(ChainList):\n",
    "    def __init__(self,in_channels,outsize):\n",
    "    #def __init__(self):\n",
    "        super(Model, self).__init__(\n",
    "            L.Convolution2D(1,32,9,pad = 4, nobias = True), #0\n",
    "            L.BatchNormalization(32), #1\n",
    "            L.Convolution2D(32,64,3,stride = 2, pad=1,nobias=True), #2\n",
    "            L.BatchNormalization(64), #3\n",
    "            L.Convolution2D(64,128,3,stride = 2, pad=1,nobias=True), #4\n",
    "            L.BatchNormalization(128), #5\n",
    "            ResidualBlock(128,128), #6\n",
    "            ResidualBlock(128,128), #7\n",
    "            ResidualBlock(128,128), #8\n",
    "            ResidualBlock(128,128), #9\n",
    "            ResidualBlock(128,128), #10\n",
    "            L.Deconvolution2D(128, 64, 3, 4, 1, True, outsize), #11\n",
    "            L.BatchNormalization(64), #12\n",
    "            L.Deconvolution2D(64, 32, 3,1, 1, True, outsize), #13\n",
    "            L.BatchNormalization(32), #14\n",
    "            L.Convolution2D(32, 3, 9, pad = 4, nobias = True), #15\n",
    "            L.BatchNormalization(3), #16     \n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h = self[0](x)\n",
    "        h = self[1](h)\n",
    "        h = F.relu(h)\n",
    "        h = self[2](h)\n",
    "        h = self[3](h)\n",
    "        h = F.relu(h)\n",
    "        h = self[4](h)\n",
    "        h = self[5](h)\n",
    "        h = F.relu(h)\n",
    "        h = self[6](h)\n",
    "        h = self[7](h)\n",
    "        h = self[8](h)\n",
    "        h = self[9](h)\n",
    "        h = self[10](h)\n",
    "        h = self[11](h)\n",
    "        h = self[12](h)\n",
    "        h = F.relu(h)\n",
    "        h = self[13](h)\n",
    "        h = self[14](h)\n",
    "        h = F.relu(h)\n",
    "        h = self[15](h)\n",
    "        h = self[16](h)\n",
    "        y = 127.5 * F.tanh(h) + 127.5\n",
    "\n",
    "        return y\n",
    "\n",
    "class ResidualBlock(Chain):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.batchNormalization_0 = L.BatchNormalization(out_channels)\n",
    "            self.batchNormalization_1 = L.BatchNormalization(out_channels)\n",
    "            self.convolution2D_0 = L.Convolution2D( in_channels, out_channels, 3, pad = 1, nobias = True)\n",
    "            self.convolution2D_1 = L.Convolution2D(out_channels, out_channels, 3, pad = 1, nobias = True)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def __call__(self, x, finetune = False):\n",
    "    \n",
    "        h = self.convolution2D_0(x)\n",
    "        h = self.batchNormalization_0(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_1(h);\n",
    "        h = self.batchNormalization_1(h);\n",
    "        h = F.relu(h)\n",
    "        y = h + x\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euDbOQWT1UA8"
   },
   "source": [
    "**Loss classes (45 points)**  \n",
    "In the following cell, you will implement the loss classes.  \n",
    "*Tasks*  \n",
    "- (1) You are provided with a custom VGG-16 implementation. How does it differ than the original implementation? Why can we get away with using the simpler implementation? (**5 points**)\n",
    "- (2) Implement the missing convolution layer of the total variation loss by filling in the missing code. (**10 points**)\n",
    "- (3) Implement the forward pass of the total variation loss by filling in the missing code. (**10 points**)\n",
    "- (4) Implement the feature loss component in the forward pass of the loss function by filling in the missing code. (**10 points**)\n",
    "- (5) Explain why the loss components are scaled. (**5 points**)\n",
    "- (6) Explain why the target features are extracted in test mode. (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DqHGhS_1M_x"
   },
   "outputs": [],
   "source": [
    "class Vgg4Layers(Chain):\n",
    "    def __init__(self):\n",
    "        super(Vgg4Layers, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "\n",
    "            \n",
    "            self.conv1_1 = L.Convolution2D( 3, 64, 3, pad = 1)\n",
    "            self.conv1_2 = L.Convolution2D( 64, 64, 3, pad = 1)\n",
    "            self.conv2_1 = L.Convolution2D( 64, 128, 3, pad = 1)\n",
    "            self.conv2_2 = L.Convolution2D(128, 128, 3, pad = 1)\n",
    "            \n",
    "\n",
    "        #add_persistent('mean', np.array([[[[103.939]], [[116.779]], [[123.68]]]]), 'float32')\n",
    "        self.mean = np.array([[[[103.939]],[[116.779]],[[ 123.68]]]], dtype='float32')\n",
    "        #self.add_persistent('mean', np.array([[[[103.939]], [[116.779]], [[123.68]]]]), 'float32')\n",
    "\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        print(type(x))\n",
    "        print(\"x.shape =\")\n",
    "        print(x.shape)\n",
    "        h = x - F.broadcast_to(self.mean, x.shape) \n",
    "        \n",
    "        h = self.conv1_1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv1_2(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.max_pooling_2d(h, 2, 2)\n",
    "        h = self.conv2_1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2_2(h)\n",
    "        y = F.relu(h)\n",
    "\n",
    "        return y\n",
    "\n",
    "    \n",
    "    \n",
    "class TotalVariationLoss(Chain):\n",
    "    def __init__(self):\n",
    "        super(TotalVariationLoss, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.convolution2D_0 = L.Convolution2D(3, 1, 2, nobias = True, initialW = np.array([3 * [[[-1], [1]]]], 'float32'))\n",
    "            # (2) start\n",
    "            self.convolution2D_1 = L.Convolution2D(3, 1, 2, nobias = True, initialW = np.array([3 * [[[1, -1]]]], 'float32'))\n",
    "            # (2) end\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # (3) start\n",
    "        h0=np.square(self.convolution2D_0(x))\n",
    "        h1=np.square(self.convolution2D_1(x))\n",
    "        \n",
    "        h=np.sum((h0,h1))\n",
    "        y=h**0.5\n",
    "        \n",
    "        # y = ...\n",
    "        # (3) end\n",
    "        print (y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    \n",
    "class LossFunction(object):\n",
    "    def __init__(self, lambda_):\n",
    "        self.totalVariationLoss = TotalVariationLoss()\n",
    "        self.vgg4Layers         = Vgg4Layers()\n",
    "\n",
    "    def __call__(self, t, y):            \n",
    "        with chainer.using_config('train', False):\n",
    "            t_ = self.vgg4Layers(t)\n",
    "        \n",
    "        # (4) start\n",
    "        \n",
    "            y_ = self.vgg4Layers(y)\n",
    "            feature_loss=lambda_['pixel']*F.mean_squared_error(t, y)\n",
    "        # ....\n",
    "        # (4) end\n",
    "        pixel_loss = lambda_['pixel'] * F.mean_squared_error(t , y)\n",
    "        total_variation_loss = lambda_['total_variation'] * self.totalVariationLoss(y)\n",
    "        loss = feature_loss + pixel_loss + total_variation_loss\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# (1) VGG4 is using only 4 layers in contrast with the VGG16 which uses 16 layers.\n",
    "#     Because the CNN are extracting features hierarchically, the more layers are used the more\n",
    "#     abstract \"concepts\" the features will represent. So by using more layers the pixel-loss is computing\n",
    "#     and pixel-loss can not \"capture perceptual differences between output and ground-truth images\"(Johnson-Alahi-FeiFei)..\n",
    "#     So by using the feature-loss functions, based on differences between high-level image feature \n",
    "#     representations extracted from pretrained convolutional neural networks, results to the production\n",
    "#     of higher quality images (Johnson-Alahi-FeiFei)\n",
    "\n",
    "# (5) Every loss component is representing a different measure, so in order to be added they have to represent \n",
    "#     the same quantity. That is being feasible with the scaling\n",
    "# (6) We don't understand the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_nGcCNEy8p3g"
   },
   "source": [
    "**Initialization (10 points)**  \n",
    "The following cell initializes the loss function, the loss history, the model, the optimizer, the datasets and the iterators. *You do not have to make any changes to the code.*  \n",
    "*Tasks*\n",
    "- (1) Study the code and refer to the chainer docuimentation if anything is unclear. You will be expected to write similar code by yourself in the coming practicals. (**0 points**)  \n",
    "- (2) What are the boolean arguments that are passed to the SerialIterator class? (**5 points**)  \n",
    "- (3) Why is it false for the training iterator but not for other iterators? In other words, what would happen if we were to set it to false for the training iterator and true for the other iterators? (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAa-KI4W-3Mm"
   },
   "outputs": [],
   "source": [
    "lossFunction = LossFunction(lambda_)\n",
    "serializers.load_npz('{:s}/Vgg4Layers.npz'.format(model_directory), lossFunction.vgg4Layers)\n",
    "loss_history = {'training': [], 'validation': [],'test':[]}\n",
    "model = Model(3, outsize) if device < 0 else Model(3, outsize).to_gpu(device)\n",
    "optimizer = Adam()\n",
    "\n",
    "optimizer.setup(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_file = sorted(glob('{}/*.jpg'.format(data_directory)))\n",
    "\n",
    "int1 = int(.64 * len(data_file))\n",
    "int2 = int(.8 * len(data_file))\n",
    "dat1 = data_file[:int1]\n",
    "dat2 = data_file[int1:int2]\n",
    "dat3 = data_file[int2:]\n",
    "\n",
    "\n",
    "#training_set = Dataset(data_file[: int( .64 * len(data_file))])\n",
    "#validation_set = Dataset(data_file[int(.64 * len(data_file) : .8 * len(data_file))])\n",
    "#test_set = Dataset(data_file[int(.8 * len(data_file) :)])\n",
    "training_set = Dataset(dat1)\n",
    "validation_set = Dataset(dat2)\n",
    "test_set = Dataset(dat3)\n",
    "\n",
    "training_iterator = iterators.SerialIterator(training_set, batch_size, False, True)\n",
    "validation_iterator = iterators.SerialIterator(validation_set, batch_size, False, False)\n",
    "test_iterator = iterators.SerialIterator(test_set , batch_size, False, False)\n",
    "\n",
    "# (2) Booleans are always either true or false. \n",
    "# The first boolean is called repeat. \n",
    "# If it is set to true, it infinetely loops over the dataset (training_set, validation_set, or test_set)\n",
    "# If it is set to false, it stops iteration at the end of the first epoch.  \n",
    "\n",
    "# The second boolean is called shuffle. \n",
    "# If it is set to true, at the beginning of each epoch the examples are shuffled.\n",
    "# If it is set to false, the examples are extracted in the order of indexes. \n",
    "\n",
    "# (3) The second boolean is True for the training iteratior and false for the other iterators. \n",
    "# Shuffle is True during the training process to make sure that the model does not overfit to the examples.\n",
    "# Shuffle is False during the other processes to be able to reproduce results.\n",
    "# If we would change this, unintentional performance degradtion will occure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAKIEbqPFzsc"
   },
   "source": [
    "**Training and validation (20 points)**  \n",
    "In the following cell, you will train and validate your model.\n",
    "*Tasks*   \n",
    "- (1) Implement training loss estimation, backprop and parameter update. (**10 points**)\n",
    "- (2) Implement validation loss history (**5 points**)\n",
    "- (3) Implement model serialization  (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-pOSKTw0tcK"
   },
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    loss_history['training'].append(0)\n",
    "\n",
    "    for j, batch in enumerate(training_iterator):\n",
    "        with chainer.using_config('train', True): #Chainer runs in trainig mode\n",
    "            t, x = concat_examples(batch, device)\n",
    "            y = model(x)\n",
    "            # (1) start  \n",
    "            \n",
    "            y.data\n",
    "            \n",
    "            \n",
    "            # training loss estimation\n",
    "            print(\"Values before\")\n",
    "            loss = lossFunction(t, y)\n",
    "            print(\"Values afterwards\")\n",
    "            \n",
    "            \n",
    "            # backprop\n",
    "            model.cleargrads()\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            #derivative = y * (1.0 - y)\n",
    "            #def transfer_derivative(y):\n",
    "            #    return output * (1.0 - y)\n",
    "            \n",
    "            # parameter update\n",
    "            optimizer.update() \n",
    "            \n",
    "            \n",
    "            # (1) end\n",
    "\n",
    "        loss_history['training'][-1] += float(loss.data)\n",
    "\n",
    "    loss_history['training'][-1] /= j + 1\n",
    "    # (2) start\n",
    "    # ...\n",
    "    loss_history['validation'].append(0)\n",
    "    \n",
    "\n",
    "   for j, batch in enumerate(validation_iterator):\n",
    "        with chainer.using_config('train', False):\n",
    "            t, x = concat_examples(batch, device)\n",
    "            y = model(x)\n",
    "            loss = lossFunction(t, y)\n",
    "            \n",
    "        loss_history['validation'][-1] += float(loss.data)\n",
    "    loss_history['validation'][-1] /= j + 1\n",
    "\n",
    "    # ...\n",
    "    # (2) end\n",
    "    print('epoch: {:3d} / {:03d}, training loss: {} , validation loss: {}.'.format(i + 1, epochs, loss_history ['training'], loss_history['validation']))\n",
    "    np.savez('{:s}/loss_history_{:03d}.npz'.format(model_directory, epoch), loss_history)\n",
    "    # (3) start\n",
    "    serializers.save_npz('our.model', model)\n",
    "    \n",
    "    # (3) end\n",
    "    save_npz('{:s}/optimizer_{:03d}.npz'.format(model_directory, epoch), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YivB1PQ7Obh"
   },
   "source": [
    "**Test (45 points + 15 bonus points)**  \n",
    "In the following cell, you will test your model.  \n",
    "*Tasks*\n",
    "- (1) Estimate the test loss, print it and save it. (**15 points**)\n",
    "- (2) Estimate the validation metrics, print them and save them (tip: scikit-image) (**15 bonus points**)\n",
    "- (3) Plot example results (i.e., plot a few t, x and y) (**10 points**)\n",
    "- (4) Dicuss your implementation in 300 - 350 words (e.g., how good your results are, how you can improve your model, etc.) (**20 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdlnCFDS-Cdh"
   },
   "outputs": [],
   "source": [
    "# (1), (2) and (3) start\n",
    "# (1) start\n",
    "lossFunction=LossFunction(lambda_)\n",
    "serializers.load_npz('{:s}/Vgg4Layers.npz'.format(model_directory), lossFunction.vgg4Layers)\n",
    "loss_history['test'].append(0)\n",
    "#ourModel=Model(3, outsize)\n",
    "ourModel = Model(3, outsize) if device < 0 else Model(3, outsize).to_gpu(device)\n",
    "\n",
    "serializers.load_npz('{:s}/our.model'.format(model_directory),ourModel)\n",
    "print(ourModel)\n",
    "type(ourModel)\n",
    "\n",
    "for k, batch in enumerate(test_iterator):\n",
    "    with chainer.using_config('test', False):\n",
    "        print(\"-----------------------------------------\")\n",
    "        print(\" k = \")\n",
    "        print(k)\n",
    "        t, x = concat_examples(batch, device)\n",
    "        t = np.array(t)\n",
    "        y = ourModel(x)\n",
    "        loss = lossFunction(t, y)\n",
    "    loss_history['test'][-1]+=float(loss.data)\n",
    "    # PSNR Metric\n",
    "    PSNR=psnr(t,y.data,data_range=255)\n",
    "    print('PSNR=')\n",
    "    print(PSNR)\n",
    "    #imshow(PSNR)\n",
    "    # SSIM Metric\n",
    "    #SSIM=ssim(t, y.data,data_range=None,multichannel=True)\n",
    "    #print('SSIM=')\n",
    "    #print(SSIM)\n",
    "    #imshow(PSNR)\n",
    "loss_history['test'][-1]/=k+1\n",
    "print(loss_history['test'][-1])\n",
    "np.savez('{:s}/test_loss_history.npz'.format(model_directory), loss_history['test'])\n",
    "# (1) end \n",
    "# (1), (2) and (3) end\n",
    "\n",
    "# (1), (2) and (3) start\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# (1), (2) and (3) end\n",
    "\n",
    "# (4) \n",
    "#   Our model is not performing that well and therefore it could be improved very much. \n",
    "#    Our PSNR results differ between 12 and 17, while the PSNR of the convolution sketch \n",
    "#    inversion paper goes up to over 20. Another thing that has to be looked at is that \n",
    "#    the difference in our PSNR results is much larger than 0.0231 bootstrap estimate of \n",
    "#    the standard error of the mean in the paper. We are not surprised of our results and \n",
    "#    have some ideas of how we could improve the model. To begin with, we only used a very \n",
    "#    small subset of the LFW dataset which is already a subset of the CelebA dataset that \n",
    "#    is used for the model discussed during the lectures and in the paper. The LFW dataset \n",
    "#    consist of over 13.000 images, of which we only used 200. We had no other choice than \n",
    "#    taking this small subset since, as far as we know, there are not online GPU’s that can \n",
    "#    work with chainer. Therefore, we had to train the model on one of our laptops and this \n",
    "#    takes, as would be no surprise, very long. After the deadline we will try to train our \n",
    "#    model with the entire dataset, to see how much it will improve.\n",
    "#    Another way in which our model could be improved is by using more sketches types. For \n",
    "#    the model discussed in the lecture three sketch types are used, namely a line sketch, \n",
    "#    a gray scale sketch, and a color sketch. In our model we only used the line sketch, \n",
    "#    which was the way to go for this assignment. The amount of improvement caused by using \n",
    "#    multiple types of sketches is off course very small compared to the improvement that would \n",
    "#    be gained by using 65 times the amount of images we used. Furthermore, after we run the model \n",
    "#    with the complete LFW dataset, it would be more interesting to look at the results of the model \n",
    "#    and from these results we might could see more ways in which our model could be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y, cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "weeks_2_and_3_assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
