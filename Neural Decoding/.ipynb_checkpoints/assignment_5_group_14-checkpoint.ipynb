{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "q69LXWGjKTN8"
   },
   "outputs": [],
   "source": [
    "# Group number: 14\n",
    "# Veerle Schepers s1023102\n",
    "# Angeliki-Ilektra Karaiskou, s1029746\n",
    "# Lei Xiaoxuan, s1025681\n",
    "# Parsia Basimfar : s1022274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYKXVQxbZ1SJ",
    "outputId": "c103cc17-4633-4148-ead5-764ff6f003af"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "\n",
    "from chainer import backend\n",
    "from chainer import backends\n",
    "from chainer.backends import cuda\n",
    "from chainer import Function, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from chainer.backends.cuda import to_cpu\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.datasets import TupleDataset\n",
    "\n",
    "#import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZyd2ovRFa8-"
   },
   "source": [
    "**SOW-MKI49: Neural Information Processing Systems**\n",
    "\n",
    "**Assignment 5: Neural Decoding**\n",
    "\n",
    "**Total points: 100**\n",
    "\n",
    "**Author: Umut**\n",
    "\n",
    "In this assignment, you will implement a neural decoder for reconstructing perceived stimuli from brain responses. We will be using the dataset that was previously used in a number of papers. These papers along with additional lecture notes on neural decoding can be found in Brightspace. You can refer them for more details on the dataset and/or the method.\n",
    "\n",
    "The dataset contains fMRI data acquired from the early visual cortex of one subject as the subject was presented with 100 grayscale images of handwritten sixes and nines (50 sixes and 50 nines). The fMRI data has been realigned and slice time corrected. Furthermore, stimulus specific response amplitudes have been estimated with a general linear model.\n",
    "\n",
    "Let's first familiarize ourselves with the dataset. It contains a number of variables:\n",
    "\n",
    "* **X** -> This is a 100 x 784 matrix. The ith row contains the pixel values of the stimulus that was presented in the ith trial of the experiment. Note that the stimuli are 28 pixel x 28 pixel images, which were reshaped to 1 x 784 vectors.\n",
    "\n",
    "* **Y** -> This is a 100 x 3092 matrix. The ith row contains the voxel values of the responses that were measured in the ith trial of the experiment.\n",
    "\n",
    "* **X_prior** -> This is a 2000 x 784 matrix. Each row contains the pixel values of a different stimulus, which was not used in the experiment. Note that the stimuli are 28 pixel x 28 pixel images, which were reshaped to 1 x 784 vectors.\n",
    "\n",
    "Note: In the remainder of this document, we will use **x** for referring to a 784 x 1 stimulus vector and **y** for referring to a 3092 x 1 response vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t03M9EgdGwiZ"
   },
   "source": [
    "## Task 1 (10 points)\n",
    "\n",
    "* Load the dataset.\n",
    "* Visualize some of the stimuli.\n",
    "* Normalize X and Y to have zero mean and unit variance. Tip: Recall that normalization means subtracting the mean of each pixel/voxel from itself and dividing it by its standard deviation. You can use zscore function.\n",
    "* Split X and Y in two parts called X_training and X_test, and Y_training and Y_test. The training set should contain 80 stimulus-response pairs (40 pairs for sixes and 40 pairs for nines). The test set should contain 20 stimulus-response pairs (10 pairs for sixes and 10 pairs for nines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pF6EIju9FZx6",
    "outputId": "026bdb63-c29a-4e56-ec66-1eac8a9a5b7a"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Load the dataset\n",
    "dataset = loadmat('69dataset2.mat')\n",
    "\n",
    "# get the stimuli\n",
    "x_data = dataset['X']\n",
    "print(x_data.shape)\n",
    "y_data = dataset['Y']\n",
    "x_prior_data = dataset['X_prior']\n",
    "# convert stimuli to array\n",
    "x_dat = np.array(x_data)\n",
    "y_dat = np.array(y_data)\n",
    "x_prior_dat = np.array(x_prior_data)\n",
    "\n",
    "\n",
    "# convert stimuli 0, 25, 50, 75 and 99 to a 28x28 numpy array\n",
    "x_0  = np.reshape(x_dat[0],[28,28],order='F')\n",
    "x_25 = np.reshape(x_dat[25],[28,28],order='F')\n",
    "x_50 = np.reshape(x_dat[50],[28,28],order='F')\n",
    "x_75 = np.reshape(x_dat[75],[28,28],order='F')\n",
    "x_99 = np.reshape(x_dat[99],[28,28],order='F')\n",
    "\n",
    "###\n",
    "# show some input stimuli\n",
    "plt.subplot(1,10,1)\n",
    "plt.imshow(x_0 )\n",
    "plt.subplot(1,10,3)\n",
    "plt.imshow(x_25 )\n",
    "plt.subplot(1,10,5)\n",
    "plt.imshow(x_50 )\n",
    "plt.subplot(1,10,7)\n",
    "plt.imshow(x_75 )\n",
    "plt.subplot(1,10,9)\n",
    "plt.imshow(x_99 )\n",
    "\n",
    "# save some input stimuli\n",
    "plt.imsave('x_0.png', x_0)\n",
    "plt.imsave('x_25.png', x_25)\n",
    "plt.imsave('x_50.png', x_50)\n",
    "plt.imsave('x_75.png', x_75)\n",
    "plt.imsave('x_99.png', x_99)\n",
    "\n",
    "###\n",
    "# Normalize X and Y to have zero mean and unit variance. \n",
    "# Tip: Recall that normalization means \n",
    "# subtracting the mean of each pixel/voxel from itself --> dat - dat.mean()\n",
    "# and dividing it by its standard deviation. --> dat_std\n",
    "# You can use zscore function.\n",
    "\n",
    "###\n",
    "# normalize X\n",
    "# mean is not zero... are we doing something wrong?\n",
    "# print(\"x_dat mean =\", x_dat.mean())\n",
    "\n",
    "# reshape to don't get weird errors in math functions\n",
    "# compute normalized values\n",
    "# x_dat2  = np.reshape(x_dat,[78400])\n",
    "\n",
    "x_dat_mean = np.mean(x_dat,0)[None]\n",
    "x_dat_std = np.std(x_dat,0)[None]\n",
    "x_dat_norm = (x_dat - x_dat_mean) / (x_dat_std)\n",
    "x_dat_norm2 = scipy.stats.mstats.zscore(x_dat,axis=None)\n",
    "where_are_NaNs = np.isnan(x_dat_norm)\n",
    "x_dat_norm[where_are_NaNs] = 0\n",
    "\n",
    "\n",
    "###\n",
    "# Normalize Y\n",
    "\n",
    "# print(\"y_dat mean =\", (y_dat.mean())) \n",
    "\n",
    "# reshape to don't get weird errors in math functions\n",
    "# compute normalized values\n",
    "# y_dat2  = np.reshape(y_dat,[309200])\n",
    "y_dat_std = np.std(y_dat,0)[None]\n",
    "y_dat_norm = (y_dat - np.mean(y_dat,0)[None])/ (y_dat_std)\n",
    "y_dat_norm2 = scipy.stats.mstats.zscore(y_dat, axis = None)\n",
    "\n",
    "\n",
    "###\n",
    "# Normalize X_prior\n",
    "\n",
    "# x_prior_dat2  = np.reshape(x_prior_dat,[1568000])\n",
    "x_prior_dat_std = np.std(x_prior_dat,0)[None]\n",
    "x_prior_dat_norm = (x_prior_dat - np.mean(x_prior_dat,0)[None]) / (x_prior_dat_std)\n",
    "\n",
    "\n",
    "where_are_NaNs = np.isnan(x_prior_dat_norm)\n",
    "x_prior_dat_norm[where_are_NaNs] = 0\n",
    "\n",
    "\n",
    "###\n",
    "# Split X and Y in two parts called X_training and X_test, and Y_training and Y_test. \n",
    "# The training set should contain 80 stimulus-response pairs (40 pairs for sixes and 40 pairs for nines).\n",
    "# The test set should contain 20 stimulus-response pairs (10 pairs for sixes and 10 pairs for nines).\n",
    "\n",
    "## do we need to use dat_norm ?\n",
    "\n",
    "### X \n",
    "# use the first 40 6's (0-39) and first 40 9's (50-89) for the training set [0:40] takes 0:39\n",
    "# combine these 2 sets with np.concatenate\n",
    "\n",
    "# Train\n",
    "x_train_6 = x_dat_norm[0:40]\n",
    "x_train_9 = x_dat_norm[50:90]\n",
    "x_train = np.concatenate((x_train_6, x_train_9), axis=0)\n",
    "\n",
    "# Test\n",
    "x_test_6 = x_dat_norm[40:50]\n",
    "x_test_9 = x_dat_norm[90:100]\n",
    "x_test = np.concatenate((x_test_6, x_test_9), axis=0)\n",
    "\n",
    "### Y\n",
    "# Train\n",
    "y_train_6 = y_dat_norm[0:40]\n",
    "y_train_9 = y_dat_norm[50:90]\n",
    "y_train = np.concatenate((y_train_6, y_train_9), axis=0)\n",
    "\n",
    "# Test\n",
    "y_test_6 = y_dat_norm[40:50]\n",
    "y_test_9 = y_dat_norm[90:100]\n",
    "y_test = np.concatenate((y_test_6, y_test_9), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7w58SydkZ1Sh"
   },
   "source": [
    "Our goal is to solve the problem of reconstructing **x** from **y**. One possible approach to solve this problem is to use a *discriminative* model. Discriminative models predict **x** as a function of **y**. That is:\n",
    "\n",
    "**x** = f(**y**)\n",
    "\n",
    "We will assume that f is a linear function. That is:\n",
    "\n",
    "**x** = **B'** **y**\n",
    "\n",
    "f can be seen as a very simple linear neural network comprising one layer of weights (i.e., **B**). We can estimate **B** in close form with ridge regression. That is:\n",
    "\n",
    "**B** = inv(**Y**\\_training' **Y**\\_training + lambda **I**) **Y**\\_training' **X**\\_training\n",
    "\n",
    "where lambda is the regularization coefficient, **I** is the *q* x *q* identity matrix, and *q* is the number of voxels. Note that we can safely ignore the intercept since we normalized our data to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewW8yODUIVbP"
   },
   "source": [
    "## Task 2  (15 points)\n",
    "\n",
    "* Estimate **B** on the training set. Tip: Normally, you should use cross validation to estimate lambda. For simplicity, you can assume that lambda = 10 ^ -6.\n",
    "* Reconstruct **x** from **y** in the test set.\n",
    "* Visualize the reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRxkSzGyH8Bh",
    "outputId": "f26f502e-003c-4123-9655-a2f98ad0e27a"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "###\n",
    "# Estimate B on the training set.\n",
    "\n",
    "# B = inv(Y_training' * Y_training + lambda * eye(q)) \\ Y_training' * X_training\n",
    "# B^= (y_train^T * y_train + lam* I*)^(-1) \\ y_train^T * x_train\n",
    "# I* = eye(q) = an q-by-q identity matrix with ones on the main diagonal and zeros elsewhere.\n",
    "\n",
    "# Y_training' = y_train^T = y_train_T = transpose y_train\n",
    "\n",
    "# given variables\n",
    "q = len(y_dat[0])\n",
    "lam = 10**-6\n",
    "\n",
    "# compute other variables\n",
    "I_q = np.eye(q)\n",
    "y_train_T = y_train.transpose()\n",
    "\n",
    "# Compute B\n",
    "temp1 = np.matmul(y_train_T,y_train) # y_train^T * y_train\n",
    "temp2 = lam * I_q # lambda * I\n",
    "part1 = (temp1 + temp2) # inv(y_train^T * y_train + lambda * I)\n",
    "part2 = np.matmul(y_train_T,x_train) # y_train_T * x_train\n",
    "B = np.linalg.solve(part1,part2) # inv(y_train^T * y_train + lambda * I) /  y_train_T * x_train\n",
    "\n",
    "\n",
    "###\n",
    "# Reconstruct x** from **y in the test set.\n",
    "# x** = **B' y\n",
    "# x_train[x] = B^T * y_train\n",
    "\n",
    "# Transpose B\n",
    "B_T = B.transpose()\n",
    "print(B_T.shape)\n",
    "x_recon = B_T.dot(y_train_T)\n",
    "x_recon = x_recon.transpose()\n",
    "\n",
    "###\n",
    "# Visualize the reconstructions\n",
    "#x = reshape(X(i, :) .* sigma + mu, 28, 28);\n",
    "# reshape the reconstructed x's\n",
    "x_recon_0  = np.reshape(np.multiply(x_recon[0],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "x_recon_19  = np.reshape(np.multiply(x_recon[19],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "x_recon_39  = np.reshape(np.multiply(x_recon[39],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "x_recon_59  = np.reshape(np.multiply(x_recon[59],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "x_recon_79  = np.reshape(np.multiply(x_recon[79],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "\n",
    "# show the reconstructed x's \n",
    "plt.figure()\n",
    "plt.imshow(x_recon_0 )\n",
    "plt.figure()\n",
    "plt.imshow(x_recon_19 )\n",
    "plt.figure()\n",
    "plt.imshow(x_recon_39 )\n",
    "plt.figure()\n",
    "plt.imshow(x_recon_59 )\n",
    "plt.figure()\n",
    "plt.imshow(x_recon_79 )\n",
    "\n",
    "# save the reconstructed x's\n",
    "plt.imsave('x_recon_0.png', x_recon_0)\n",
    "plt.imsave('x_recon_19.png', x_recon_19 )\n",
    "plt.imsave('x_recon_39.png', x_recon_39 )\n",
    "plt.imsave('x_recon_59.png', x_recon_59 )\n",
    "plt.imsave('x_recon_79.png', x_recon_79 )\n",
    "\n",
    "\n",
    "\n",
    "### show multiple figures\n",
    "# to check if they are different\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure()\n",
    "#fig.add_subplot(5,1,1)\n",
    "plt.figure()\n",
    "plt.imshow(x_recon_0 )\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(x_recon_19 )\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(x_recon_39 )\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(x_recon_59 )\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(x_recon_79 )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYLXZGJ4H8LP"
   },
   "source": [
    "\n",
    "Another possible approach to solve the problem of reconstructing **x** from **y** is to use a *generative* model and invert it with Bayes' rule. We reformulate the problem as finding the most probable **x** that could have caused **y**. That is:\n",
    "\n",
    "argmax_**x** P(**x** | **y**)\n",
    "\n",
    "where P(**x** | **y**) is called the posterior (probability of the stimulus being **x** if the observation is **y**). In other words, we have to define the posterior, estimate its parameters and find the argument that maximizes it, which will be the reconstruction of **x** from **y**. While, this may seem daunting, it actually has a simple solution. The posterior assigns a probability to an event by combining our observations and beliefs about it, and can be decomposed with Bayes' rule as the product of how likely our observations are given the event (probability of observing **y** if the stimulus is **x**) and how likely the event is independent of our observations (probability of the stimulus being **x**). That is:\n",
    "\n",
    "P(**x** | **y**) ~ P(**y** | **x**) * P(**x**)\n",
    "\n",
    "where P(**y** | **x**) is called the likelihood and P(x) is called the prior.\n",
    "\n",
    "We will assume that the likelihood and the prior are multivariate Gaussian distributions. A Gaussian is characterized by two parameters: a mean vector and a covariance matrix. In the case of the likelihood, the mean of the Gaussian is given by:\n",
    "\n",
    "**mu**\\_likelihood = **B'** **x**\n",
    "\n",
    "As before, we can estimate **B** in close form with ridge regression:\n",
    "\n",
    "**B** = inv(**X**\\_training' **X**\\_training + lambda **I**) **X**\\_training' **Y**\\_training\n",
    "\n",
    "where lambda is the regularization coefficient, I is the *p* x *p* identity matrix, and *p* is the number of pixels. The covariance matrix of the likelihood is given by:\n",
    "\n",
    "**Sigma**_likelihood = diag(E[||**y** - **B'** **x**|| ^ 2]). \n",
    "\n",
    "In the case of the prior, the mean of the Gaussian is given by:\n",
    "\n",
    "**mu**\\_prior = **0** (which is a vector of zeros)\n",
    "\n",
    "The covariance matrix of the prior is given by:\n",
    "\n",
    "**Sigma**\\_prior = **X**\\_prior' * **X**\\_prior / (n - 1)\n",
    "\n",
    "where n is the length of **X**\\_prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITe4_zh1JZn1"
   },
   "source": [
    "## Task 3 (20 points)\n",
    "\n",
    "* Estimate **B** on the training set. Tip: Normally, you should use cross-validation to estimate lambda and Sigma_likelihood. For simplicity, you can assume that lambda = 10 ^ -6 and Sigma_likelihood = 10 ^ -3 **I**.\n",
    "* Estimate **Sigma**\\_prior. Tip: Add 10 ^ -6 to the diagonal of Sigma_prior for regularization.\n",
    "* Visualize **Sigma**\\_prior. Can you explain what it shows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vePaLa1sJiFB",
    "outputId": "d924fbe9-3c45-4381-f2be-a445c98f51dc"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Estimate B** on the training set.\n",
    "#B = inv(X_training' X_training + lambda I) X_training' Y_training\n",
    "# mu_likelihood = B' x\n",
    "x_train_T = x_train.transpose()\n",
    "mu_likelihood = np.matmul(B,x_train_T) # B'x\n",
    "p = len(x_dat[0]) # number of pixels = 28*28 = 784\n",
    "\n",
    "I_p = np.eye(p) \n",
    "\n",
    "Sigma_likelihood = 10**-3* I_q # size 3092X3092\n",
    "\n",
    "# compute B\n",
    "#B               = (X_training' * X_training + lambda * eye(p)) \\  X_training' * Y_training;\n",
    "#   \n",
    "B_gen_temp1 = np.matmul(x_train_T,x_train) # X_training' **X_training\n",
    "B_gen_temp2 = lam * I_p #  lambda I*\n",
    "B_gen_part2 = np.matmul(x_train_T,y_train) # X_training' Y_training\n",
    "B_gen2 = np.linalg.solve(B_gen_temp1 + B_gen_temp2, B_gen_part2)\n",
    "print(B_gen2.shape)\n",
    "\n",
    "# B** = inv(X_training' **X_training + lambda I*) *X_training' Y_training\n",
    "\n",
    "###\n",
    "# Estimate Sigma_prior\n",
    "# Tip: Add 10 ^ -6 to the diagonal of Sigma_prior for regularization.\n",
    "\n",
    "# Sigma_prior = X*_prior' * *X_prior / (n - 1)\n",
    "# where n is the length of X_prior.\n",
    "n = len(x_prior_dat[0])\n",
    "\n",
    "x_prior = x_prior_dat_norm\n",
    "x_prior_T = x_prior.transpose()\n",
    "\n",
    "Sigma_temp1 = x_prior_T.dot(x_prior)\n",
    "Sigma_temp2 = (n - 1)\n",
    "Sigma_temp3 = Sigma_temp1 / Sigma_temp2\n",
    "\n",
    "\n",
    "\n",
    "Sigma_prior = Sigma_temp3 + lam * I_p\n",
    "\n",
    "### Visualize Sigma_prior. \n",
    "### Can you explain what it shows?\n",
    "\n",
    "\n",
    "plt.imshow(Sigma_prior)\n",
    "plt.imsave('Sigma_prior.png', Sigma_prior)\n",
    "\n",
    "###\n",
    "# Sigma_prior, in the linear Gaussian case, \n",
    "# is the required covariance matrix for the prior R\n",
    "# R = 1 (N-1) * sum(z^n*(z^n)^T)\n",
    "# Schoenmakers (2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PMMnQfeJidL"
   },
   "source": [
    "Having defined the likelihood and the prior as Gaussians, we can derive the posterior by multiplying them. It turns out that the product of two Gaussians is another Gaussian, whose mean vector is given by:\n",
    "\n",
    "**mu**\\_posterior = inv(inv(**Sigma**\\_prior) + **B** inv(**Sigma**\\_likelihood) **B**') **B** * inv(**Sigma**\\_likelihood) **y**\n",
    "\n",
    "We are almost done. Recall that the reconstruction of **x** from **y** is the argument that maximizes the posterior, which we derived to be a Gaussian. We will be completely done once we answer the following question: What is the argument that maximizes a Gaussian?\n",
    "\n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".\n",
    "\n",
    "The answer is its mean vector, which is the solution of our initial problem. That is:\n",
    "\n",
    "argmax_**x** P(**x** | **y**) =  \n",
    "**mu**\\_posterior =  \n",
    "inv(inv(**Sigma**\\_prior) + **B** inv(**Sigma**\\_likelihood) **B**') **B** * inv(**Sigma**\\_likelihood) **y**\n",
    "\n",
    "Now, we can plug any **y** in the above equation and reconstruct the most probable **x** that could have caused it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pr4X7qQSZ1S4",
    "outputId": "7800675a-2040-443b-f264-6e34bee961bb"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TLn3_7q2J93x"
   },
   "source": [
    "## Task 4 (25 points)\n",
    "\n",
    "- Reconstruct **x** from **y** in the test set.\n",
    "- Visualize the reconstructions.\n",
    "- Compare the reconstructions with the earlier reconstructions. Which one is better? Why? Can you think of ways to improve the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1Y9vcw-Z1TA",
    "outputId": "ba12ba99-8120-4302-ae05-68dca0f6f3a4"
   },
   "outputs": [],
   "source": [
    "\n",
    "inv_sig_prior = np.linalg.inv(Sigma_prior) # inv(Sigma_prior)\n",
    "B_gen2_T = B_gen2.T\n",
    "#B inv(Sigma_likelihood) B'\n",
    "B_Sl_BT=np.matmul(np.linalg.solve(Sigma_likelihood.T, B_gen2.T).T,B_gen2.T)\n",
    "arg_inv=inv_sig_prior+B_Sl_BT\n",
    "arg2=np.linalg.solve( Sigma_likelihood.T,B_gen2.T).T\n",
    "arg3=np.linalg.solve(arg_inv,arg2)\n",
    "final=np.matmul(arg3,y_test.T)\n",
    "\n",
    "\n",
    "\n",
    "x_test_recon = final.T\n",
    "print(x_test_recon.shape)\n",
    "x_test_recon_T = x_test_recon.transpose()\n",
    "print(x_test_recon_T.shape)\n",
    "\n",
    "\n",
    "###\n",
    "# Visualize the reconstructions.\n",
    "\n",
    "# reshape the reconstructed x's\n",
    "x_test_recon_0  = np.reshape(np.multiply(x_test_recon[15],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "x_test_recon_4  = np.reshape(np.multiply(x_test_recon[4],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "x_test_recon_9  = np.reshape(np.multiply(x_test_recon[10],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "x_test_recon_14  = np.reshape(np.multiply(x_test_recon[14],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "x_test_recon_19  = np.reshape(np.multiply(x_test_recon[19],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "plt.figure()\n",
    "print(x_test_recon_0.shape)\n",
    "#fig=plt.figure()\n",
    "for i in range(0,20):\n",
    "# show the reconstructed x's \n",
    "    plt.figure()\n",
    "    x_test_recon_=np.reshape(np.multiply(x_test_recon[i],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "    plt.imshow(x_test_recon_)\n",
    "    \n",
    "#plt.show()\n",
    "\n",
    "# save the reconstructed x's\n",
    "plt.imsave('x_test_recon_0.png', x_test_recon_0 )\n",
    "plt.imsave('x_test_recon_4.png', x_test_recon_4 )\n",
    "plt.imsave('x_test_recon_9.png', x_test_recon_9 )\n",
    "plt.imsave('x_test_recon_14.png', x_test_recon_14 )\n",
    "plt.imsave('x_test_recon_19.png', x_test_recon_19 )\n",
    "\n",
    "\n",
    "###\n",
    "# Compare the reconstructions with the earlier reconstructions. \n",
    "# Which one is better? Why? Can you think of ways to improve the results?\n",
    "\n",
    "\n",
    "# The earlier version was better. This is logical since it is trained on the 80 train-sets instead of the 20 test-sets\n",
    "# Improvement can be done by using the larger test-set to learn and then test it on the smaller test-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQwBsKfdKzJc"
   },
   "source": [
    "Now the real fight begins! You might have already realized that the second approach is very similar to the one in the face reconstruction paper that we have discussed in the last lecture. The main difference is how the stimuli are encoded. So far, we have been encoding the stimuli with an identity transformation:\n",
    "\n",
    "**mu**\\_likelihood = **B'** g(**x**)\n",
    "\n",
    "where g(**x**) = **x**\n",
    "\n",
    "In the paper, they are encoded as the features extracted from a DNN:\n",
    "\n",
    "**mu**\\_likelihood = **B'** DNN(**x**)\n",
    "\n",
    "Similarly, we have been decoding the responses with MAP estimation of the stimuli:\n",
    "\n",
    "g ^ -1(argmax_g(**x**) P(g(**x**) | **y**))\n",
    "\n",
    "where g ^ -1(**y**) = **y**\n",
    "\n",
    "In the paper, they are decoded with MAP estimation of the features followed by an inverse DNN:\n",
    "\n",
    "DNN ^ -1(argmax_DNN(**x**) P(DNN(**x**) | **y**))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6prNNFgwN9jA"
   },
   "source": [
    "## Task 5 (30 points)\n",
    "\n",
    "- Train a simple DNN for digit classification on the MNIST dataset.\n",
    "- Select an indermediate layer of the DNN as your feature extractor.\n",
    "- Train another simple DNN to invert the first DNN. That is, it should transform features extracted by the selected layer of the first DNN to stimuli.\n",
    "- Repeat Task 3 but use the trained DNNs instead of identity transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Bmy81JzZ1TN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7821a0392579>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmax_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m## we are doing digit classification instead of construting the brain stimuli\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwithlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchainer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miterators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chainer' is not defined"
     ]
    }
   ],
   "source": [
    "batchsize = 128\n",
    "max_epoch = 10\n",
    "## we are doing digit classification instead of construting the brain stimuli\n",
    "train, test = chainer.datasets.mnist.get_mnist(withlabel=True, ndim=3)\n",
    "from chainer import iterators, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dba445a294f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mVVVV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mVV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVVVV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Bmy81JzZ1TN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8c86cca3bef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mtrain_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "[train_temp,temp] = dataset.concat_examples(train)\n",
    "[test_temp,temp] = dataset.concat_examples(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Bmy81JzZ1TN"
   },
   "outputs": [],
   "source": [
    "# create a dataset iterator\n",
    "train_iter = iterators.SerialIterator(train,batchsize)\n",
    "test_iter = iterators.SerialIterator(test,batchsize,repeat = False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFKZ75PyZ1TV"
   },
   "outputs": [],
   "source": [
    "class DNN(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(None, 8, ksize=3)\n",
    "            self.conv2 = L.Convolution2D(None, 16, ksize=3)\n",
    "            self.l3 = L.Linear(None, 32)\n",
    "            self.l4 = L.Linear(None, 10)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        h1 = self.conv1(x)\n",
    "        h1 = F.relu(h1)\n",
    "        h2 = self.conv2(h1)\n",
    "        h2 = F.relu(h2)\n",
    "        h3 = self.l3(h2)\n",
    "        h3 = F.relu(h3)\n",
    "        h4 = self.l4(h3)\n",
    "        #h4 = F.relu(h4)\n",
    "        return h4\n",
    "\n",
    "    def extract(self, x, layer_name=None):\n",
    "        \n",
    "        if layer_name is None:\n",
    "            IOError('abort!')\n",
    "\n",
    "        if layer_name == 'conv1':\n",
    "            h1 = self.conv1(x)\n",
    "            h1 = F.relu(h1)\n",
    "            return h1\n",
    "        \n",
    "        elif layer_name == 'conv2':\n",
    "            h1 = self.conv1(x)\n",
    "            h1 = F.relu(h1)\n",
    "            h2 = self.conv2(h1)\n",
    "            h2 = F.relu(h2)\n",
    "            return h2\n",
    "        \n",
    "        elif layer_name == 'l3':\n",
    "            h1 = self.conv1(x)\n",
    "            h1 = F.relu(h1)\n",
    "            h2 = self.conv2(h1)\n",
    "            h2 = F.relu(h2)\n",
    "            h3 = self.l3(h2)\n",
    "            h3 = F.relu(h3)\n",
    "            return h3\n",
    "        \n",
    "        elif layer_name == 'l4':\n",
    "            h1 = self.conv1(x)\n",
    "            h1 = F.relu(h1)\n",
    "            h2 = self.conv2(h1)\n",
    "            h2 = F.relu(h2)\n",
    "            h3 = self.l3(h2)\n",
    "            h3 = F.relu(h3)\n",
    "            h4 = self.l4(h3)\n",
    "            #h4 = F.relu(h4)\n",
    "            return h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN()\n",
    "\n",
    "gpu_id = -1\n",
    "if gpu_id >= 0:\n",
    "    model.to_gpu(gpu_id)\n",
    "else:\n",
    "    model.to_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ucqkwvu2Z1Tc",
    "outputId": "c0e75e19-1372-4418-9ed5-1cb78a1fbfc5"
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(alpha=0.1)\n",
    "optimizer.setup(model)\n",
    "\n",
    "# loss function: softmax_cross_entropy()\n",
    "while train_iter.epoch < max_epoch:\n",
    "    # iteration of the training loop\n",
    "    train_batch = train_iter.next()\n",
    "    image_train, target_train = concat_examples(train_batch,gpu_id)\n",
    "    \n",
    "    prediction_train = model(image_train)\n",
    "    \n",
    "    loss = F.softmax_cross_entropy(prediction_train,target_train)\n",
    "    \n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.update()\n",
    "    \n",
    "    # check the validation accuracy of predictions after every epoch\n",
    "    if train_iter.is_new_epoch:\n",
    "        # show the training loss\n",
    "        print('epoch:{:02d} train_loss:{:.04f}'.format(train_iter.epoch,float(to_cpu(loss.data))),end='')\n",
    "        \n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        while True:\n",
    "            test_batch = test_iter.next()\n",
    "            image_test,target_test = concat_examples(test_batch,gpu_id)\n",
    "            prediction_test = model(image_test)\n",
    "            loss_test = F.softmax_cross_entropy(prediction_test,target_test)\n",
    "            test_losses.append(to_cpu(loss_test.data))\n",
    "            \n",
    "            accuracy = F.accuracy(prediction_test,target_test)\n",
    "            accuracy.to_cpu()\n",
    "            test_accuracies.append(accuracy.data)\n",
    "            \n",
    "            if test_iter.is_new_epoch:\n",
    "                test_iter.epoch=0\n",
    "                test_iter.current_position=0\n",
    "                test_iter.is_new_epoch=False\n",
    "                test_iter._pushed_position = None\n",
    "                break\n",
    "        print('Val_loss:{:.04f} val_accuracy:{:.04f}'.format(np.mean(test_losses),np.mean(test_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HqGcSHlcZ1Tj"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "serializers.save_npz('DNN_MNIST_model_3.npz',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "9DPg_QVlZ1Tm",
    "outputId": "f01246ea-5aa6-4b88-8140-187eaff7ae9d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "## perform the DNN classification model\n",
    "dnn_model = DNN()\n",
    "serializers.load_npz('DNN_MNIST_model_3.npz',dnn_model)\n",
    "# get a test image and label\n",
    "x,t = test[5964]\n",
    "plt.imshow(x.reshape(28,28))\n",
    "print('label:',t)\n",
    "# use the model\n",
    "x = x[None,...]\n",
    "y = dnn_model(x)\n",
    "y = y.data\n",
    "# look for the most probable digit number using argmax\n",
    "prediction_label = y.argmax(axis=1)\n",
    "print(\"preticted label:\",prediction_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vbDfk5XJZ1Tu",
    "outputId": "04f01a65-af3d-41e3-817b-799b3d213c85"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-de5ed768dd9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# how to extract the corresponding latent feature of all stimuli???\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnum_tr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mnum_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'num_tr ='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'num_test ='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_temp' is not defined"
     ]
    }
   ],
   "source": [
    "## select an intermediate layer\n",
    "layer_name = 'conv2'\n",
    "#layer_name = 'l3'\n",
    "#layer_name = 'L2'\n",
    "# how to extract latent feature\n",
    "# how to extract the corresponding latent feature of all stimuli???\n",
    "num_tr=60000\n",
    "num_test=len(test_temp)\n",
    "print('num_tr =', num_tr)\n",
    "print('num_test =',num_test)\n",
    "\n",
    "latent_feature_tr = []\n",
    "latent_feature_te = []\n",
    "i=0\n",
    "\n",
    "for img_tr in train_temp[:num_tr]:\n",
    "    \n",
    "    latent_feature_tr.append(dnn_model.extract(np.expand_dims(img_tr, axis=0), layer_name=layer_name).data)\n",
    "\n",
    "for img_te in test_temp[:num_test]:\n",
    "    latent_feature_te.append(dnn_model.extract(np.expand_dims(img_te, axis=0), layer_name=layer_name).data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vbDfk5XJZ1Tu",
    "outputId": "04f01a65-af3d-41e3-817b-799b3d213c85"
   },
   "outputs": [],
   "source": [
    "latent_feature_tr_n = latent_feature_tr\n",
    "latent_feature_te_n  = latent_feature_te\n",
    "latent_feature_tr = np.array(latent_feature_tr)\n",
    "latent_feature_te = np.array(latent_feature_te)\n",
    "\n",
    "\n",
    "inv_train = TupleDataset(latent_feature_tr, train_temp[:num_tr])\n",
    "inv_test = TupleDataset(latent_feature_te,test_temp[:num_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJs6DC8RZ1T0"
   },
   "outputs": [],
   "source": [
    "## invert dnn\n",
    "# input: latent_feature\n",
    "# output: image/stimuli 28*28\n",
    "\n",
    "# \n",
    "\n",
    "class invert_DNN(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(invert_DNN,self).__init__()\n",
    "        with self.init_scope():\n",
    "\n",
    "            self.l0 = L.Linear(None, 128)\n",
    "            self.l1 = L.Linear(128, 300)\n",
    "            self.l2 = L.Linear(300,500)\n",
    "            self.l3 = L.Linear(500,784)\n",
    "\n",
    "    def __call__(self,x):\n",
    "  \n",
    "        h1 = self.l0(x)\n",
    "        h1 = F.relu(h1)\n",
    "        h5 = self.l1(h1)\n",
    "        h5 = F.relu(h5)\n",
    "        h6 = self.l2(h5)\n",
    "        h6 = F.relu(h6)\n",
    "        h7 = self.l3(h6)\n",
    "        h7 = F.sigmoid(h7)\n",
    "\n",
    "        return h7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bv3teu_dZ1T3"
   },
   "outputs": [],
   "source": [
    "# traning of the new DNN\n",
    "import IPython\n",
    "batchsize = 50\n",
    "gpu_id = -1\n",
    "inv_train_iter = iterators.SerialIterator(inv_train,batchsize,repeat = False, shuffle=False)\n",
    "inv_test_iter = iterators.SerialIterator(inv_test,batchsize,repeat = False, shuffle=False)\n",
    "\n",
    "\n",
    "inv_model = invert_DNN()\n",
    "inv_model.to_cpu()\n",
    "inv_optimizer = optimizers.SGD(lr=0.5)\n",
    "inv_optimizer.setup(inv_model)\n",
    "dnn_model.disable_update()\n",
    "\n",
    "loss_history = {'training': [],'validation' : []}\n",
    "#loss_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bv3teu_dZ1T3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs =100\n",
    "for i in range(epochs):\n",
    "    loss_history['training'].append(0)\n",
    "    loss_history['validation'].append(0)\n",
    "    inv_train_iter.reset()\n",
    "    #validation_iterator.reset()\n",
    "    for j, batch in enumerate(inv_train_iter):\n",
    "        with chainer.using_config('train', True):\n",
    "            # print('j = ', j)\n",
    "            #inv_model.cleargrads()\n",
    "            feature_train, stimuli_train = concat_examples(batch,-1)\n",
    "            x=chainer.Variable(feature_train)\n",
    "            stimuli_train=chainer.Variable(np.reshape(stimuli_train,[batchsize,784]))\n",
    "\n",
    "            predicted = inv_model(x)\n",
    "\n",
    "            \n",
    "            loss = F.mean_squared_error(predicted,stimuli_train)\n",
    "            inv_model.cleargrads()\n",
    "            loss.backward()\n",
    "\n",
    "            inv_optimizer.update()\n",
    "        \n",
    "            loss_history['training'][-1] += float(loss.data)\n",
    "            #loss_history['training'].append(float(loss.data))\n",
    "            #print(float(loss.data))\n",
    "    loss_history['training'][-1] /= j + 1\n",
    "    \n",
    "    #loss_history['validation'].append(0)\n",
    "\n",
    "\n",
    "    for j1, batch1 in enumerate(inv_test_iter):\n",
    "        with chainer.using_config('train', False):\n",
    "            feature_train1, stimuli_train1 = concat_examples(batch1,-1)\n",
    "            x1=chainer.Variable(feature_train1)\n",
    "            stimuli_train1=chainer.Variable(np.reshape(stimuli_train1,[batchsize,784]))\n",
    "            #x1 = chainer.Variable(dataset.concat_examples(batch1))\n",
    "            predicted1 = inv_model(x1)\n",
    "\n",
    "\n",
    "            loss = F.mean_squared_error(predicted1,stimuli_train1)\n",
    "\n",
    "            #loss = loss_fn(mean_z1, ln_var_z1,x1, mean_x1, ln_var_x1)\n",
    "            inv_model.cleargrads()\n",
    "            loss.backward()\n",
    "\n",
    "            inv_optimizer.update()\n",
    "            loss_history['validation'][-1] += float(loss.data)\n",
    "    loss_history['validation'][-1] /= j1 + 1\n",
    "    \n",
    "    print(\"iteration i = \")\n",
    "    print(i)\n",
    "    print(\"is ended and we are going to the next one\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    #print('epoch: {:3d} / {:03d}, training loss: {:.4f} , validation loss: {:.4f}.'.format(i + 1, epochs, loss_history ['training'][i], loss_history['validation'][i]))\n",
    "    print('epoch: {:3d} / {:03d}, training loss: {:.4f} '.format(i + 1, epochs, loss_history ['training'][i]))\n",
    "np.savez('{:s}loss_history_{:03d}.npz'.format('', epochs), loss_history)\n",
    "serializers.save_npz('group14_invDNN_50epochs_V1.model', inv_model)\n",
    "\n",
    "serializers.save_npz('{:s}optimizer_{:03d}.npz'.format('', epochs), inv_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(loss_history))\n",
    "train = plt.plot(np.arange(100), loss_history['training'], label = 'training loss')\n",
    "\n",
    "plt.legend(['training loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn=DNN()\n",
    "serializers.load_npz('DNN_MNIST_model_3.npz',dnn)\n",
    "inv_DNN = invert_DNN()\n",
    "serializers.save_npz('group14_invDNN_50epochs_V1.model',inv_DNN)\n",
    "\n",
    "## select an intermediate layer\n",
    "layer_name = 'conv2'\n",
    "#layer_name = 'l3'\n",
    "#layer_name = 'L2'\n",
    "# how to extract latent feature\n",
    "# how to extract the corresponding latent feature of all stimuli???\n",
    "num_tr=len(x_train)\n",
    "num_test=len(x_test)\n",
    "num_prior=len(x_prior)\n",
    "\n",
    "x_feat_train = []\n",
    "x_feat_test = []\n",
    "x_feat_prior = []\n",
    "\n",
    "x_prior_n = np.reshape(x_prior,[num_prior,1,28,28])\n",
    "x_prior_n = x_prior_n.astype(np.float32)\n",
    "\n",
    "x_train_n = np.reshape(x_train,[num_tr,1,28,28])\n",
    "x_train_n = x_train_n.astype(np.float32)\n",
    "\n",
    "x_test_n = np.reshape(x_test,[num_test,1,28,28])\n",
    "x_test_n = x_test_n.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "for img_tr in x_train_n[:num_tr]:\n",
    "    x_feat_train.append(dnn.extract(np.expand_dims(img_tr, axis=0), layer_name=layer_name).data)\n",
    "\n",
    "for img_te in x_test_n[:num_test]:\n",
    "    x_feat_test.append(dnn.extract(np.expand_dims(img_te, axis=0), layer_name=layer_name).data)\n",
    "    \n",
    "for img_prior in x_prior_n[:num_prior]:\n",
    "    x_feat_prior.append(dnn.extract(np.expand_dims(img_prior, axis=0), layer_name=layer_name).data)   \n",
    "    \n",
    "x_feat_train_n = x_feat_train\n",
    "x_feat_test_n  = x_feat_test\n",
    "x_feat_prior_n = x_feat_prior\n",
    "x_feat_train = np.array(x_feat_train)\n",
    "x_feat_test = np.array(x_feat_test)\n",
    "x_feat_prior = np.array(x_feat_prior)\n",
    "\n",
    "print(x_feat_train[0].shape)\n",
    "arithmos=x_feat_train[0].shape\n",
    "print(arithmos)\n",
    "\"\"\"inv_x_train = TupleDataset(x_feat_train, x_train_n[:num_tr])\n",
    "inv_x_test = TupleDataset(x_feat_test,x_test_n[:num_test])   \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.reshape(x_feat_train,[80,-1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inv_x_train[0].type)\n",
    "xf_train = np.reshape(x_feat_train,[80,-1])\n",
    "xf_trainT = xf_train.transpose()\n",
    "#print(xf_train.shape,xf_trainT.shape)\n",
    "xf_prior = np.reshape(x_feat_prior,[len(x_prior),-1])\n",
    "xf_priorT = xf_prior.transpose()\n",
    "#print(xf_prior.shape,xf_priorT.shape)\n",
    "#import sys\n",
    "#print (sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mu_likelihood = np.matmul(B,x_train_T) # B'x\n",
    "p = len(xf_train[0]) # number of pixels = 28*28 = 784\n",
    "\n",
    "I_p = np.eye(p) \n",
    "\n",
    "Sigma_likelihood = 10**-3* I_q # size 3092X3092\n",
    "\n",
    "# compute B\n",
    "#B               = (X_training' * X_training + lambda * eye(p)) \\  X_training' * Y_training;\n",
    "#   \n",
    "B_gen_temp1 = np.matmul(xf_trainT,xf_train) # X_training' **X_training\n",
    "B_gen_temp2 = lam * I_p #  lambda I*\n",
    "B_gen_part2 = np.matmul(xf_trainT,y_train) # X_training' Y_training\n",
    "#print(B_gen_temp1.shape,B_gen_temp2.shape,B_gen_part2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_gen2 = np.linalg.solve(B_gen_temp1 + B_gen_temp2, B_gen_part2)\n",
    "#print(B_gen2.shape)\n",
    "\n",
    "# B** = inv(X_training' **X_training + lambda I*) *X_training' Y_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Estimate Sigma_prior\n",
    "# Tip: Add 10 ^ -6 to the diagonal of Sigma_prior for regularization.\n",
    "\n",
    "# Sigma_prior = X*_prior' * *X_prior / (n - 1)\n",
    "# where n is the length of X_prior.\n",
    "nf = len(xf_prior[0])\n",
    "#print(nf)\n",
    "#print(xf_priorT.shape,xf_prior.shape)\n",
    "Sigma_tempf1 = xf_priorT.dot(xf_prior)\n",
    "Sigma_tempf2 = (nf - 1)\n",
    "Sigma_tempf3 = Sigma_tempf1 / Sigma_tempf2\n",
    "\n",
    "Sigma_priorf = Sigma_tempf3 + lam * I_p\n",
    "\n",
    "plt.imshow(Sigma_prior)\n",
    "plt.imsave('Sigma_prior.png', Sigma_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_sig_prior = np.linalg.inv(Sigma_priorf) # inv(Sigma_prior)\n",
    "B_gen2_T = B_gen2.T\n",
    "#B inv(Sigma_likelihood) B'\n",
    "B_Sl_BT=np.matmul(np.linalg.solve(Sigma_likelihood.T, B_gen2.T).T,B_gen2.T)\n",
    "arg_inv=inv_sig_prior+B_Sl_BT\n",
    "arg2=np.linalg.solve( Sigma_likelihood.T,B_gen2.T).T\n",
    "arg3=np.linalg.solve(arg_inv,arg2)\n",
    "final=np.matmul(arg3,y_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_recon_f = final.T\n",
    "print(x_test_recon_f.shape)\n",
    "x_test_recon_f_T = x_test_recon_f.transpose()\n",
    "print(x_test_recon_f_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.reshape(x_test_recon_f,[20, 1, 32]).shape) #linear layer\n",
    "x_test_recon_f = np.reshape(x_test_recon_f,[20, 1, 16, 24, 24]).astype(np.float32) #conv2 layer\n",
    "#x_test_recon_f = np.reshape(x_test_recon_f,[20, 1, 32]).astype(np.float32) # l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test_recon_f[0].shape)\n",
    "print(x_test_recon_f[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_recon_final=[]\n",
    "for img_feat in range(len(x_test_recon_f)):\n",
    "    \n",
    "    x=chainer.Variable(x_test_recon_f[img_feat])\n",
    "    #print(x)\n",
    "    predicted = inv_DNN(x)\n",
    "    x_test_recon_final.append(np.array(predicted.data).astype(np.float64))\n",
    "\n",
    "x_test_recon_final=np.array(x_test_recon_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test_recon_final.shape)\n",
    "plt.figure()\n",
    "#x_test_recon_0  = np.reshape(np.multiply(x_test_recon_final[7],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "x_test_recon_0  = np.reshape(x_test_recon_final[7],[28,28],order='F')\n",
    "plt.imshow(x_test_recon_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "# show the reconstructed x's \n",
    "    plt.figure()\n",
    "    x_test_recon_=np.reshape(np.multiply(x_test_recon_final[i],x_dat_std)+x_dat_mean,[28,28],order='F')\n",
    "    plt.imshow(x_test_recon_)\n",
    "    \n",
    "for i in range(0,20):\n",
    "# show the reconstructed x's \n",
    "    plt.figure()\n",
    "    x_test_recon_=np.reshape(x_test_recon_final[i],[28,28],order='F')\n",
    "    plt.imshow(x_test_recon_)    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "assignment_5_V17_Xiao_extract2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
